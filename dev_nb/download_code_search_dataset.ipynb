{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.concat([pd.read_csv(f'https://storage.googleapis.com/kubeflow-examples/code_search/raw_data/00000000000{i}.csv') \\\n",
    "#                 for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tqdm(total=10) as t:\n",
    "#     t.set_description(f\"Training\")\n",
    "#     for i in range(10):\n",
    "#         t.set_postfix({\"epoch\": i})\n",
    "#         t.update(10)\n",
    "#         time.sleep(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100it [00:21, 34.08it/s]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.set_description(f\"eoch {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     url = f\"https://storage.googleapis.com/kubeflow-examples/code_search/raw_data/00000000000{i}.csv\"\n",
    "#     print(url)\n",
    "#     !wget {url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.core import *\n",
    "from fastai.basics import *\n",
    "path = \"/mnt/hdd/datasets/google-code-search/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path/'000000000000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head(1).values[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\n",
      ".. py:module:: fnl.text.dictionary\n",
      "   :synopsis: Recognize terms in token streams and tag them with their mapped keys.\n",
      "\n",
      ".. moduleauthor:: Florian Leitner <florian.leitner@gmail.com>\n",
      ".. License: GNU Affero GPL v3 (http://www.gnu.org/licenses/agpl.html)\n",
      "\"\"\"\n",
      "import logging\n",
      "\n",
      "from fnl.nlp.strtok import Tokenizer\n",
      "\n",
      "\n",
      "class Node(object):\n",
      "\t\"\"\"\n",
      "\tNodes in a Dictionary tree point to child Nodes via token-tagged edges\n",
      "\tand possibly have a Leaf that maps to a Dictionary key.\n",
      "\t\"\"\"\n",
      "\n",
      "\tdef __init__(self, *leafs, **edges):\n",
      "\t\tself.edges = edges\n",
      "\t\tself.leafs = sorted(leafs)\n",
      "\n",
      "\tdef __eq__(self, other):\n",
      "\t\tif isinstance(other, Node):\n",
      "\t\t\treturn id(self) == id(other) or (self.leafs == other.leafs and self.edges == other.edges)\n",
      "\t\telse:\n",
      "\t\t\treturn False\n",
      "\n",
      "\tdef __repr__(self):\n",
      "\t\treturn \"Node<leafs={}, edges={}>\".format(self.leafs, self.edges)\n",
      "\n",
      "\tdef createOrGet(self, token):\n",
      "\t\t\"\"\"\n",
      "\t\tCreate or get the Node pointed to by `token` from this node.\n",
      "\n",
      "\t\t:param token: edge label\n",
      "\t\t:return: the child :class:`Node` instance\n",
      "\t\t\"\"\"\n",
      "\t\tif token in self.edges:\n",
      "\t\t\tnode = self.edges[token]\n",
      "\t\telse:\n",
      "\t\t\tnode = Node()\n",
      "\t\t\tself.edges[token] = node\n",
      "\n",
      "\t\treturn node\n",
      "\n",
      "\tdef setLeaf(self, key, order):\n",
      "\t\t\"\"\"\n",
      "\t\tStore the `key` as a leaf of this node at position `order`.\n",
      "\n",
      "\t\tThe key is the main key if there is no leaf yet,\n",
      "\t\tor if `order` is smaller than all the existing leafs' order values.\n",
      "\t\tOtherwise, it the key is stored as one of the alternative keys.\n",
      "\t\tAt equal order values, keys are sorted in lexical order.\n",
      "\n",
      "\t\t:param key: to store\n",
      "\t\t:param order: value used to sort/compare keys (smaller first)\n",
      "\t\t\"\"\"\n",
      "\t\tself.leafs.append((order, key))\n",
      "\t\tself.leafs = sorted(self.leafs)\n",
      "\n",
      "\t@property\n",
      "\tdef key(self):\n",
      "\t\t\"\"\"Return the main key of this leaf.\"\"\"\n",
      "\t\treturn self.leafs[0][1] if self.leafs else None\n",
      "\n",
      "\n",
      "class Dictionary(object):\n",
      "\t\"\"\"\n",
      "\tDictionaries are trees of token-edges where Nodes at the end of token paths\n",
      "\tthat represent a Dictionary term map to the term's key using a Leaf object.\n",
      "\t\"\"\"\n",
      "\n",
      "\tB = 'B-%s'\n",
      "\tI = 'I-%s'\n",
      "\tO = 'O'\n",
      "\tlogger = logging.getLogger('fnl.text.dictionary.Dictionary')\n",
      "\n",
      "\t@staticmethod\n",
      "\tdef merge(node1, node2) -> Node:\n",
      "\t\t\"\"\"\n",
      "\t\tMerge two nodes to produce a new, united node with all their children.\n",
      "\n",
      "\t\t:param node1: to merge\n",
      "\t\t:param node2: to merge\n",
      "\t\t:return: a merged node, always a copy\n",
      "\t\t\"\"\"\n",
      "\t\tif node1 == node2:\n",
      "\t\t\treturn Node(*node1.leafs, **node1.edges)\n",
      "\n",
      "\t\tedges = node1.edges.copy()\n",
      "\n",
      "\t\tfor e in node2.edges:\n",
      "\t\t\tif e in edges:\n",
      "\t\t\t\tedges[e] = Dictionary.merge(edges[e], node2.edges[e])\n",
      "\t\t\telse:\n",
      "\t\t\t\tedges[e] = node2.edges[e]\n",
      "\n",
      "\t\treturn Node(*sorted(node1.leafs + node2.leafs), **edges)\n",
      "\n",
      "\tdef __init__(self, data: iter, tokenizer: Tokenizer):\n",
      "\t\t\"\"\"\n",
      "\t\tInitialize a new Dictionary using a data iterator and a (term) tokenizer.\n",
      "\n",
      "\t\t:param data: an iterator over (key, term, *order) tuples\n",
      "\t\t:param tokenizer: to tokenize terms\n",
      "\t\t\"\"\"\n",
      "\t\tself.root = Node()\n",
      "\n",
      "\t\tfor key, term, *order in data:\n",
      "\t\t\tnode = self.root\n",
      "\n",
      "\t\t\tfor start, end, tag, morphology in tokenizer.tokenize(term):\n",
      "\t\t\t\t# special matching condition: single letter match\n",
      "\t\t\t\t# can use both cases inside an already opened match\n",
      "\t\t\t\tif start > 0 and end - start == 1:\n",
      "\t\t\t\t\tnode = node.createOrGet(term[start:end].lower())\n",
      "\t\t\t\telse:\n",
      "\t\t\t\t\tnode = node.createOrGet(term[start:end])\n",
      "\n",
      "\t\t\tnode.setLeaf(key, order)\n",
      "\n",
      "\tdef walk(self, token_stream: iter) -> iter:\n",
      "\t\t\"\"\"\n",
      "\t\tYield a stream of \"B-\"/\"I-\" prefixed keys for each token that matches\n",
      "\t\t(part of) a term or \"O\" if no match is found for the current token.\n",
      "\n",
      "\t\tA matching term starts with \"B-[key]\", continues with \"I-[key]\",\n",
      "\t\tand ends with an \"O\" (or the stream itself ends).\n",
      "\t\tThat means that per token only the first-best matching term is reported.\n",
      "\n",
      "\t\t:param token_stream: token strings to match with the dictionary\n",
      "\t\t:return: BIO-key strings, one per token string\n",
      "\t\t\"\"\"\n",
      "\t\tqueue = []\n",
      "\t\tlast = None\n",
      "\n",
      "\t\tfor token in token_stream:\n",
      "\t\t\tqueue = self._match(queue, token, last)\n",
      "\t\t\tyield from self._iterpop(queue)\n",
      "\t\t\tlast = token\n",
      "\n",
      "\t\t# close all paths\n",
      "\t\tfor idx in range(len(queue)):\n",
      "\t\t\tpath = queue[idx]\n",
      "\n",
      "\t\t\tif path is not None:\n",
      "\t\t\t\tqueue[idx] = tuple(path)\n",
      "\n",
      "\t\tyield from self._iterpop(queue, True)\n",
      "\n",
      "\tdef _iterpop(self, queue, all=False) -> iter:\n",
      "\t\t\"\"\"\n",
      "\t\tYield tags on the queue that contain values.\n",
      "\n",
      "\t\t:param queue: to process\n",
      "\t\t:return: a tag generator\n",
      "\t\t\"\"\"\n",
      "\t\tcount = 0 if all else 1\n",
      "\t\twhile len(queue) > count:\n",
      "\t\t\tif queue[0] is None:\n",
      "\t\t\t\tqueue.pop(0)\n",
      "\t\t\t\tyield Dictionary.O\n",
      "\t\t\telif type(queue[0]) is tuple:\n",
      "\t\t\t\tfor tag in self._resolve(queue.pop(0), queue):\n",
      "\t\t\t\t\tyield tag\n",
      "\t\t\telse:\n",
      "\t\t\t\tbreak\n",
      "\n",
      "\t@staticmethod\n",
      "\tdef _isCapitalized(token):\n",
      "\t\t# last may be whatever kind of alphabetic token\n",
      "\t\treturn len(token) > 1 and token.isalpha() and token[0].isupper() and token[1:].islower()\n",
      "\n",
      "\t@staticmethod\n",
      "\tdef _isCapitalizeD(last, token):\n",
      "\t\t# last may be whatever kind of alphabetic token\n",
      "\t\treturn last and len(token) == 1 and last.isalpha() and token.isupper()\n",
      "\n",
      "\tdef _mergeAlt(self, alt, queue):\n",
      "\t\t# No check of len(queue) required:\n",
      "\t\t# if this fails, something is wrong with _iterpop,\n",
      "\t\t# because it should guarantee that at least the last item is left\n",
      "\t\tlast_path = queue[-1]\n",
      "\n",
      "\t\tif last_path is None:\n",
      "\t\t\t# create a path if none exists\n",
      "\t\t\tlast_path = queue[-1] = []\n",
      "\t\telif type(last_path) == tuple:\n",
      "\t\t\t# reopen closed paths\n",
      "\t\t\tlast_path = queue[-1] = list(last_path)\n",
      "\n",
      "\t\tn = self.root.edges[alt]\n",
      "\n",
      "\t\tif len(last_path):\n",
      "\t\t\tassert len(last_path) != 1, \"merging 2-token alt on a path of length 1\"\n",
      "\t\t\tlast_path[-2] = Dictionary.merge(last_path[-2], n)\n",
      "\t\t\tlast_path[-1] = Dictionary.merge(last_path[-1], n)\n",
      "\t\t\tself.logger.debug(\"merge alt token '%s'\", alt)\n",
      "\t\telse:\n",
      "\t\t\tlast_path.append(Node(**n.edges))\n",
      "\t\t\tlast_path.append(n)\n",
      "\t\t\tself.logger.debug(\"open alt token '%s'\", alt)\n",
      "\n",
      "\tdef _match(self, queue, token, last):\n",
      "\t\t# alt: joins the current token with the last if the current token is\n",
      "\t\t# a single upper-case letter and the last token is alphabetic,\n",
      "\t\t# but then upper-casing all letters\n",
      "\t\talt = \"{}{}\".format(last, token).upper() if Dictionary._isCapitalizeD(last, token) else None\n",
      "\t\t# upper: if the token is all lowercase, create a pure uppercase version\n",
      "\t\tupper = token.upper() if token.islower() else None\n",
      "\t\t# lower: if the token is capitalized and not a single letter, create a lower-case version\n",
      "\t\tlower = token.lower() if Dictionary._isCapitalized(token) else None\n",
      "\n",
      "\t\tfor idx in range(len(queue)):\n",
      "\t\t\tif queue[idx] is None or type(queue[idx]) is tuple:\n",
      "\t\t\t\tcontinue\n",
      "\n",
      "\t\t\tpath = queue[idx]  # the current path that may be extended\n",
      "\t\t\tedges = path[-1].edges  # all outgoing edges on that path that can be used to extend it\n",
      "\t\t\taltNode = path[-2] if alt and len(path) > 1 else self.root  # the alternative path\n",
      "\n",
      "\t\t\tif token in edges:\n",
      "\t\t\t\tif alt in altNode.edges:\n",
      "\t\t\t\t\tpath.append(Dictionary.merge(edges[token], altNode.edges[alt]))\n",
      "\t\t\t\t\tself.logger.debug(\"match cont'd token %i '%s' and alt '%s'\",\n",
      "\t\t\t\t\t                  len(path), token, alt)\n",
      "\t\t\t\telse:\n",
      "\t\t\t\t\tpath.append(edges[token])\n",
      "\t\t\t\t\tself.logger.debug(\"match cont'd token %i '%s'\", len(path), token)\n",
      "\t\t\telif len(token) == 1 and token.isupper() and token.lower() in edges:\n",
      "\t\t\t\t# special matching condition: single letter match\n",
      "\t\t\t\t# with swapped case inside an already opened path\n",
      "\t\t\t\tpath.append(edges[token.lower()])\n",
      "\t\t\t\tself.logger.debug(\"match cont'd single letter %i '%s'\", len(path), token.lower())\n",
      "\t\t\telif alt in altNode.edges:\n",
      "\t\t\t\t# allow joint token matches if the second token is a single, upper-case letter\n",
      "\t\t\t\t# and the first token was a letter token beginning with upper-case, too\n",
      "\t\t\t\tpath.append(altNode.edges[alt])\n",
      "\t\t\t\tself.logger.debug(\"match cont'd alt %i '%s'\", len(path), alt)\n",
      "\t\t\telif upper and upper in edges:\n",
      "\t\t\t\t# allow full-token lower-case to upper-case transitions\n",
      "\t\t\t\t# to detect mentions of genes written in all lower-case\n",
      "\t\t\t\tpath.append(edges[upper])\n",
      "\t\t\t\tself.logger.debug(\"match cont'd upper %i '%s'\", len(path), upper)\n",
      "\t\t\telif lower and lower in edges:\n",
      "\t\t\t\t# allow full-token capitalized to lower-case transitions\n",
      "\t\t\t\t# to detect mentions of gene tokens written in all lower-case\n",
      "\t\t\t\tpath.append(edges[lower])\n",
      "\t\t\t\tself.logger.debug(\"match cont'd lower %i '%s'\", len(path), lower)\n",
      "\t\t\telse:\n",
      "\t\t\t\t# \"close\" this path\n",
      "\t\t\t\tqueue[idx] = tuple(path)\n",
      "\t\t\t\tself.logger.debug(\"match closed at token %i '%s'\", len(path), token)\n",
      "\n",
      "\t\t# \"open\" a new path if the token matches an edge in root\n",
      "\t\tif token in self.root.edges:\n",
      "\t\t\tif upper and upper in self.root.edges:\n",
      "\t\t\t\tqueue.append([Dictionary.merge(self.root.edges[token], self.root.edges[upper])])\n",
      "\t\t\t\tself.logger.debug(\"match open token '%s' and upper '%s'\", token, upper)\n",
      "\t\t\telif lower and lower in self.root.edges:\n",
      "\t\t\t\tqueue.append([Dictionary.merge(self.root.edges[token], self.root.edges[lower])])\n",
      "\t\t\t\tself.logger.debug(\"match open token '%s' and lower '%s'\", token, lower)\n",
      "\t\t\telif alt in self.root.edges:\n",
      "\t\t\t\tself._mergeAlt(alt, queue)\n",
      "\t\t\t\tqueue.append([self.root.edges[token]])\n",
      "\t\t\t\tself.logger.debug(\"match open token '%s' and merge alt token '%s'\", token, alt)\n",
      "\t\t\telse:\n",
      "\t\t\t\tqueue.append([self.root.edges[token]])\n",
      "\t\t\t\tself.logger.debug(\"match open token '%s'\", token)\n",
      "\t\telif upper and upper in self.root.edges:\n",
      "\t\t\tqueue.append([self.root.edges[upper]])\n",
      "\t\t\tself.logger.debug(\"match open upper token '%s'\", upper)\n",
      "\t\telif lower and lower in self.root.edges:\n",
      "\t\t\t# allow capitalized token to lower-case transitions at first token\n",
      "\t\t\t# to detect mentions of capitalized gene names\n",
      "\t\t\tqueue.append([self.root.edges[lower]])\n",
      "\t\t\tself.logger.debug(\"match open lower token '%s'\", lower)\n",
      "\t\telse:\n",
      "\t\t\tif alt in self.root.edges:\n",
      "\t\t\t\tself._mergeAlt(alt, queue)\n",
      "\t\t\t\tself.logger.debug(\"merge alt token '%s'\", alt)\n",
      "\n",
      "\t\t\tqueue.append(None)  # nothing (no start) found at the current token\n",
      "\n",
      "\t\treturn queue\n",
      "\n",
      "\tdef _resolve(self, path, queue) -> iter:\n",
      "\t\tfor node in reversed(path):\n",
      "\t\t\tif node.key:\n",
      "\t\t\t\tself.logger.debug(\"found %s (%i tokens)\", node.key, len(path))\n",
      "\t\t\t\tidx = 0\n",
      "\t\t\t\tikey = Dictionary.I % node.key\n",
      "\t\t\t\tyield Dictionary.B % node.key\n",
      "\n",
      "\t\t\t\twhile path[idx] != node:\n",
      "\t\t\t\t\tyield ikey\n",
      "\t\t\t\t\tidx += 1\n",
      "\t\t\t\t\t# overlapping terms are dropped\n",
      "\t\t\t\t\tqueue.pop(0)\n",
      "\n",
      "\t\t\t\treturn\n",
      "\n",
      "\t\t# the path did not contain a key\n",
      "\t\tyield Dictionary.O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(df.head(1).values[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
